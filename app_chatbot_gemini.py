from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
from sentence_transformers import SentenceTransformer, util
import json
import re

from config import load_config
from gemini_ai import configure_gemini, generate_sql_query, generate_natural_language_response
from db import get_db_connection
from schema_utils import load_schema, extract_table_names, validate_tables_in_sql, extract_possible_table_names, filter_schema_by_table_names
from sql_utils import is_safe_sql
from token_utils import token_manager

conversation_memory = {}
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

with open("table_keywords.json", "r", encoding="utf-8") as f:
    keyword_table_mapping = json.load(f)

def find_similar_cached_response(user_id, new_question, threshold=0.85):
    if user_id not in conversation_memory:
        return None
    
    new_embedding = embedding_model.encode(new_question, convert_to_tensor=True)
    for item in reversed(conversation_memory[user_id][-5:]):  # ch·ªâ check 5 c√¢u g·∫ßn nh·∫•t
        sim = util.cos_sim(new_embedding, item['embedding'])[0][0].item()
        if sim > threshold:
            return item  # Tr·∫£ l·∫°i k·∫øt qu·∫£ c≈© ƒë√£ truy v·∫•n
    
    return None

def parse_tasks_from_system(tasks_text):
    """
    Parse danh s√°ch task t·ª´ h·ªá th·ªëng
    Format: "ID - ProjectName - TaskName"
    """
    tasks = []
    lines = tasks_text.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Pattern: "221 - HousingStaff - Mockup register"
        match = re.match(r'(\d+)\s*-\s*([^-]+)\s*-\s*(.+)', line)
        if match:
            task_id, project_name, task_name = match.groups()
            tasks.append({
                'id': task_id.strip(),
                'project_name': project_name.strip(),
                'task_name': task_name.strip()
            })
    
    return tasks

def parse_daily_report(report_text):
    """
    Parse b√°o c√°o h·∫±ng ng√†y ƒë·ªÉ extract th√¥ng tin effort
    Ch·ªâ ph√¢n t√≠ch ph·∫ßn "Today(Actual - Th·ª±c t·∫ø)"
    """
    # T√¨m ph·∫ßn Actual
    actual_pattern = r'‚ñ†‚ñ†\s*Today\(Actual\s*-\s*Th·ª±c t·∫ø\)\s*‚ñ†‚ñ†(.*?)(?=‚ñ†‚ñ†|$)'
    actual_match = re.search(actual_pattern, report_text, re.DOTALL | re.IGNORECASE)
    
    if not actual_match:
        return []
    
    actual_section = actual_match.group(1)
    projects_effort = []
    
    # Pattern ƒë·ªÉ t√¨m project v√† effort: "‚ñ† ProjectName - Xh"
    project_pattern = r'‚ñ†\s*([^-]+)\s*-\s*(\d+)h'
    project_matches = re.findall(project_pattern, actual_section, re.IGNORECASE)
    
    for project_name, effort in project_matches:
        project_name = project_name.strip()
        effort = int(effort)
        
        # T√¨m c√°c task trong project n√†y
        tasks = []
        # Pattern ƒë·ªÉ t√¨m task: "+ Task description"
        task_pattern = r'\+\s*(.+)'
        task_matches = re.findall(task_pattern, actual_section)
        
        # T√¨m ph·∫ßn task thu·ªôc v·ªÅ project n√†y
        project_start = actual_section.find(f"‚ñ† {project_name}")
        if project_start != -1:
            # T√¨m ph·∫ßn k·∫øt th√∫c c·ªßa project (d√≤ng ti·∫øp theo b·∫Øt ƒë·∫ßu b·∫±ng ‚ñ†)
            next_project = re.search(r'\n\s*‚ñ†\s*[^-]', actual_section[project_start:])
            if next_project:
                project_section = actual_section[project_start:project_start + next_project.start()]
            else:
                project_section = actual_section[project_start:]
            
            # Extract tasks t·ª´ ph·∫ßn project
            task_matches = re.findall(r'\+\s*(.+)', project_section)
            tasks = [task.strip() for task in task_matches]
        
        projects_effort.append({
            'project_name': project_name,
            'effort': effort,
            'tasks': tasks
        })
    
    return projects_effort

def match_tasks_with_system_tasks(daily_efforts, system_tasks):
    """
    Match effort t·ª´ b√°o c√°o v·ªõi task trong h·ªá th·ªëng
    C·∫£i thi·ªán logic ƒë·ªÉ ch·∫•p nh·∫≠n task c√≥ √Ω nghƒ©a t∆∞∆°ng t·ª±
    """
    results = []
    undefined_tasks = []
    
    for daily_effort in daily_efforts:
        project_name = daily_effort['project_name']
        total_effort = daily_effort['effort']
        daily_tasks = daily_effort['tasks']
        
        # T√¨m c√°c task trong h·ªá th·ªëng thu·ªôc v·ªÅ project n√†y
        matching_system_tasks = [
            task for task in system_tasks 
            if task['project_name'].lower() == project_name.lower()
        ]
        
        # N·∫øu kh√¥ng t√¨m th·∫•y task trong h·ªá th·ªëng theo project name
        if not matching_system_tasks:
            # Th·ª≠ t√¨m task theo n·ªôi dung t∆∞∆°ng t·ª± trong to√†n b·ªô h·ªá th·ªëng
            matched_tasks = []
            unmatched_daily_tasks = []
            
            for task_desc in daily_tasks:
                best_match = None
                best_score = 0
                
                for sys_task in system_tasks:
                    # T√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng d·ª±a tr√™n nhi·ªÅu ti√™u ch√≠
                    score = calculate_similarity_score(task_desc, sys_task['task_name'])
                    if score > best_score and score >= 0.3:  # Ng∆∞·ª°ng t·ªëi thi·ªÉu 30%
                        best_score = score
                        best_match = sys_task
                
                if best_match:
                    matched_tasks.append({
                        'daily_task': task_desc,
                        'system_task': best_match,
                        'score': best_score
                    })
                else:
                    unmatched_daily_tasks.append(task_desc)
            
            # N·∫øu t√¨m th·∫•y √≠t nh·∫•t 1 task t∆∞∆°ng t·ª±, ph√¢n b·ªï effort
            if matched_tasks:
                effort_per_matched_task = total_effort / len(matched_tasks)
                for match in matched_tasks:
                    results.append(f"{match['system_task']['id']} - {effort_per_matched_task}")
                
                # Th√™m c√°c task kh√¥ng match v√†o undefined
                for task_desc in unmatched_daily_tasks:
                    undefined_tasks.append({
                        'project_name': project_name,
                        'task_name': task_desc,
                        'effort': total_effort / len(daily_tasks) if daily_tasks else total_effort
                    })
            else:
                # Kh√¥ng t√¨m th·∫•y task n√†o t∆∞∆°ng t·ª±, th√™m t·∫•t c·∫£ v√†o undefined
                for task_desc in daily_tasks:
                    undefined_tasks.append({
                        'project_name': project_name,
                        'task_name': task_desc,
                        'effort': total_effort / len(daily_tasks) if daily_tasks else total_effort
                    })
            continue
        
        # N·∫øu c√≥ task trong h·ªá th·ªëng theo project name
        matched_daily_tasks = []
        unmatched_daily_tasks = []
        
        for task_desc in daily_tasks:
            best_match = None
            best_score = 0
            
            # ∆Øu ti√™n t√¨m trong c√πng project tr∆∞·ªõc
            for sys_task in matching_system_tasks:
                score = calculate_similarity_score(task_desc, sys_task['task_name'])
                if score > best_score and score >= 0.3:
                    best_score = score
                    best_match = sys_task
            
            # N·∫øu kh√¥ng t√¨m th·∫•y trong c√πng project, t√¨m trong to√†n b·ªô h·ªá th·ªëng
            if not best_match:
                for sys_task in system_tasks:
                    score = calculate_similarity_score(task_desc, sys_task['task_name'])
                    if score > best_score and score >= 0.3:
                        best_score = score
                        best_match = sys_task
            
            if best_match:
                matched_daily_tasks.append({
                    'daily_task': task_desc,
                    'system_task': best_match,
                    'score': best_score
                })
            else:
                unmatched_daily_tasks.append(task_desc)
        
        # Ph√¢n b·ªï effort cho c√°c task ƒë√£ match
        if matched_daily_tasks:
            effort_per_matched_task = total_effort / len(matched_daily_tasks)
            for match in matched_daily_tasks:
                results.append(f"{match['system_task']['id']} - {effort_per_matched_task}")
        
        # Th√™m c√°c task kh√¥ng match v√†o undefined
        for task_desc in unmatched_daily_tasks:
            undefined_tasks.append({
                'project_name': project_name,
                'task_name': task_desc,
                'effort': total_effort / len(daily_tasks) if daily_tasks else total_effort
            })
    
    return results, undefined_tasks

def calculate_similarity_score(daily_task, system_task):
    """
    T√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng gi·ªØa task trong b√°o c√°o v√† task trong h·ªá th·ªëng
    Tr·∫£ v·ªÅ ƒëi·ªÉm t·ª´ 0-1, c√†ng cao c√†ng t∆∞∆°ng t·ª±
    """
    daily_lower = daily_task.lower()
    system_lower = system_task.lower()
    
    # 1. Exact match ho·∫∑c substring match
    if daily_lower == system_lower:
        return 1.0
    if daily_lower in system_lower or system_lower in daily_lower:
        return 0.8
    
    # 2. Keyword matching
    daily_words = set(daily_lower.split())
    system_words = set(system_lower.split())
    
    # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng quan tr·ªçng
    stop_words = {'l√†m', 'vi·∫øt', 't·∫°o', 'th·ª±c', 'hi·ªán', 'c√¥ng', 'vi·ªác', 'task', 'work', 'do', 'make', 'create'}
    daily_words = daily_words - stop_words
    system_words = system_words - stop_words
    
    if not daily_words or not system_words:
        return 0.0
    
    # T√≠nh ƒëi·ªÉm d·ª±a tr√™n s·ªë t·ª´ chung
    common_words = daily_words & system_words
    if common_words:
        similarity = len(common_words) / max(len(daily_words), len(system_words))
        return min(similarity * 1.5, 1.0)  # TƒÉng ƒëi·ªÉm cho keyword matching
    
    # 3. Fuzzy matching cho c√°c t·ª´ t∆∞∆°ng t·ª±
    # V√≠ d·ª•: "mockup" ~ "mock up", "register" ~ "registration"
    similar_keywords = {
        'mockup': ['mock', 'up', 'design'],
        'register': ['registration', 'signup', 'sign'],
        'meeting': ['meet', 'discussion', 'b√°o c√°o'],
        'estimate': ['estimation', 't√≠nh to√°n'],
        'price': ['pricing', 'gi√°', 'cost'],
        'schedule': ['l·ªãch', 'tr√¨nh', 'plan'],
        'transfer': ['chuy·ªÉn', 'move', 'copy']
    }
    
    daily_keywords = set()
    system_keywords = set()
    
    for word in daily_words:
        for key, similar_list in similar_keywords.items():
            if word in similar_list or key in word:
                daily_keywords.add(key)
                break
    
    for word in system_words:
        for key, similar_list in similar_keywords.items():
            if word in similar_list or key in word:
                system_keywords.add(key)
                break
    
    if daily_keywords and system_keywords:
        common_keywords = daily_keywords & system_keywords
        if common_keywords:
            return 0.6  # ƒêi·ªÉm cho fuzzy matching
    
    return 0.0


logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

app = Flask(__name__)
CORS(app, resources={r"/ask": {"origins": "http://pms.test"}})
config_data = load_config()
configure_gemini(config_data["GEMINI_API_KEY"])
DB_CONFIG = config_data["DB"]

schema_text = load_schema()
allowed_tables = set(t.lower() for t in extract_table_names(schema_text))

def is_modifying_question(question: str) -> bool:
    # hi·ªán t·∫°i modify keywords ƒëang hard code ch·ªâ l√† m·ªôt danh s√°ch ƒë∆°n gi·∫£n, c√≥ th·ªÉ m·ªü r·ªông sau n√†y
    # ph∆∞∆°ng ph√°p m·ªü r·ªông c√≥ th·ªÉ l√† s·ª≠ d·ª•ng m√¥ h√¨nh AI ƒë·ªÉ ph√¢n t√≠ch c√¢u h·ªèi nh∆∞ng hi·ªán t·∫°i s·∫Ω t·ªën ph√≠ n√™n ch∆∞a tri·ªÉn khai
    modifying_keywords = [
        "th√™m", "t·∫°o", "ch√®n", "insert", "c·∫≠p nh·∫≠t", "update", "x√≥a", "ch·ªânh s·ª≠a", "delete", "drop", "remove", "alter", "edit", "drop"
    ]
    question_lower = question.lower()
    return any(keyword in question_lower for keyword in modifying_keywords)

@app.route("/ask", methods=["POST"])
def handle_question():
    data = request.get_json()
    question = data.get("question", "")
    # user_id = data.get("user_id", "default")  # th√™m user_id v√†o request PMS ƒë·ªÉ t√°ch session
    
    if not question:
        return jsonify({"error": "Missing question"}), 400

    if is_modifying_question(question):
        return {"error": "C√¢u h·ªèi mang t√≠nh ch·ªânh s·ª≠a d·ªØ li·ªáu. Kh√¥ng th·ª±c hi·ªán."}
    
    try:
        # # üîç Ki·ªÉm tra c√¢u h·ªèi t∆∞∆°ng t·ª±
        # cached = find_similar_cached_response(user_id, question)
        # if cached:
        #     logging.info("‚úÖ D√πng l·∫°i k·∫øt qu·∫£ t·ª´ cache")
        #     response = generate_natural_language_response(question, cached["result"])
        #     return jsonify({
        #         "question": question,
        #         "sql_generated": cached["sql"],
        #         "results": cached["result"],
        #         "response": response,
        #         "cached": True
        #     })

        # üí° Kh√¥ng c√≥ cache, ti·∫øp t·ª•c nh∆∞ hi·ªán t·∫°i
        ####### generated_sql = generate_sql_query(question, schema_text)
        # keyword -> related tables

        def guess_tables_from_question(question, keyword_mapping):
            """
            Tr·∫£ v·ªÅ danh s√°ch b·∫£ng c√≥ th·ªÉ li√™n quan ƒë·∫øn c√¢u h·ªèi d·ª±a tr√™n keyword mapping
            """
            question_lower = question.lower()
            matched_tables = set()
            for keyword, tables in keyword_mapping.items():
                if keyword in question_lower:
                    matched_tables.update(tables)
            return list(matched_tables)
        # T√¨m c√°c b·∫£ng c√≥ th·ªÉ li√™n quan ƒë·∫øn c√¢u h·ªèi
        relevant_tables = guess_tables_from_question(question, keyword_table_mapping)

# N·∫øu kh√¥ng ƒëo√°n ƒë∆∞·ª£c b·∫£ng n√†o ‚Üí fallback to√†n b·ªô schema
        if not relevant_tables:
            logging.info("Kh√¥ng t√¨m th·∫•y b·∫£ng li√™n quan, d√πng to√†n b·ªô schema")
            relevant_schema = schema_text
        else:
            logging.info(f"C√°c b·∫£ng li√™n quan ƒë·∫øn c√¢u h·ªèi: {relevant_tables}")
            relevant_schema = filter_schema_by_table_names(schema_text, relevant_tables)

        # G·ªçi Gemini sinh SQL t·ª´ schema r√∫t g·ªçn
        try:
            generated_sql = generate_sql_query(question, relevant_schema)
        except ValueError as ve:
            # Token limit exceeded
            logging.error(f"Token limit error: {str(ve)}")
            return jsonify({
                "error": "C√¢u h·ªèi qu√° ph·ª©c t·∫°p ho·∫∑c schema qu√° l·ªõn. Vui l√≤ng th·ª≠ c√¢u h·ªèi c·ª• th·ªÉ h∆°n.",
                "error_type": "token_limit_exceeded",
                "details": str(ve)
            }), 400
        except Exception as e:
            # Other API errors
            logging.error(f"Gemini API error: {str(e)}")
            return jsonify({
                "error": "C√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "error_type": "api_error"
            }), 500

        if not is_safe_sql(generated_sql):
            return jsonify({
                "error": "Ch·ªâ c√¢u h·ªèi an to√†n ƒë∆∞·ª£c ph√©p v√† ch·∫•p nh·∫≠n c√¢u h·ªèi SQL an to√†n.",
                "sql_generated": generated_sql
            }), 400

        is_valid, forbidden = validate_tables_in_sql(generated_sql, allowed_tables)
        if not is_valid:
            return jsonify({
                "error": f"Query references tables not in schema: {', '.join(forbidden)}",
                "sql_generated": generated_sql
            }), 400

        conn = get_db_connection(DB_CONFIG)
        cursor = conn.cursor(dictionary=True)
        cursor.execute(generated_sql)
        results = cursor.fetchall()
        cursor.close()
        conn.close()

        try:
            natural_response = generate_natural_language_response(question, results)
        except Exception as e:
            logging.error(f"Error generating natural response: {str(e)}")
            # Fallback response if token issues
            natural_response = f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ cho c√¢u h·ªèi c·ªßa b·∫°n. D·ªØ li·ªáu c√≥ th·ªÉ xem trong ph·∫ßn 'results'."

        # # üß† L∆∞u l·∫°i k·∫øt qu·∫£ v√†o b·ªô nh·ªõ
        # emb = embedding_model.encode(question, convert_to_tensor=True)
        # conversation_memory.setdefault(user_id, []).append({
        #     "question": question,
        #     "embedding": emb,
        #     "sql": generated_sql,
        #     "result": results
        # })

        return jsonify({
            "question": question,
            "sql_generated": generated_sql,
            "results": results,
            "response": natural_response,
            "cached": False
        })

    except Exception as e:
        logging.error(f"Error in /ask endpoint: {str(e)}")
        return jsonify({"error": str(e)}), 500

# API RI√äNG CHO SERVER CHATBOT ƒê·ªÇ T·ªêI ∆ØU HI·ªÜU SU·∫§T M√î H√åNH AI
@app.route("/cache", methods=["GET"])
def view_cache():
    user_id = request.args.get("user_id", "default")
    history = conversation_memory.get(user_id, [])

    simplified = [
        {
            "question": item["question"],
            "sql": item["sql"],
            "num_results": len(item["result"])  # ch·ªâ tr·∫£ s·ªë l∆∞·ª£ng ƒë·ªÉ tr√°nh log qu√° nhi·ªÅu
        }
        for item in history
    ]

    return jsonify({
        "user_id": user_id,
        "cached_questions": simplified,
        "total_cached": len(simplified)
    })



@app.route("/cache/clear", methods=["POST"])
def clear_cache():
    user_id = request.json.get("user_id", "default")
    conversation_memory[user_id] = []
    return jsonify({"message": f"Cache cleared for user {user_id}."})

@app.route("/token/info", methods=["GET"])
def get_token_info():
    """
    API ƒë·ªÉ xem th√¥ng tin token limits v√† stats
    """
    stats = token_manager.get_token_stats()
    
    # Th√™m th√¥ng tin v·ªÅ schema
    schema_tokens = token_manager.count_tokens(schema_text)
    
    return jsonify({
        "token_limits": stats,
        "schema_info": {
            "total_tokens": schema_tokens,
            "total_tables": len(allowed_tables),
            "schema_size_kb": len(schema_text) / 1024
        },
        "recommendations": {
            "max_question_tokens": stats["max_input_tokens"] - schema_tokens - 1000,
            "status": "healthy" if schema_tokens < stats["max_input_tokens"] * 0.7 else "warning"
        }
    })

@app.route("/timesheet-daily", methods=["POST"])
def analyze_timesheet_daily():
    """
    API ƒë·ªÉ ph√¢n t√≠ch b√°o c√°o h·∫±ng ng√†y v√† mapping v·ªõi task trong h·ªá th·ªëng
    """
    try:
        data = request.get_json()
        system_tasks_text = data.get("system_tasks", "")
        daily_report = data.get("daily_report", "")
        
        if not system_tasks_text or not daily_report:
            return jsonify({
                "error": "Thi·∫øu th√¥ng tin system_tasks ho·∫∑c daily_report"
            }), 400
        
        # Parse system tasks
        system_tasks = parse_tasks_from_system(system_tasks_text)
        logging.info(f"Parsed {len(system_tasks)} system tasks")
        
        # Parse daily report
        daily_efforts = parse_daily_report(daily_report)
        logging.info(f"Parsed {len(daily_efforts)} daily efforts")
        
        # Match tasks
        results, undefined_tasks = match_tasks_with_system_tasks(daily_efforts, system_tasks)
        
        return jsonify({
            "results": results,
            "undefined": undefined_tasks,
            "summary": {
                "total_system_tasks": len(system_tasks),
                "total_daily_efforts": len(daily_efforts),
                "matched_results": len(results),
                "undefined_tasks": len(undefined_tasks)
            }
        })
        
    except Exception as e:
        logging.error(f"Error in /timesheet-daily endpoint: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/timesheet-daily-ai", methods=["POST"])
def analyze_timesheet_daily_ai():
    """
    API s·ª≠ d·ª•ng AI (Gemini) ƒë·ªÉ ph√¢n t√≠ch b√°o c√°o timesheet v√† mapping v·ªõi task h·ªá th·ªëng
    """
    try:
        data = request.get_json()
        system_tasks_text = data.get("system_tasks", "")
        daily_report = data.get("daily_report", "")
        
        if not system_tasks_text or not daily_report:
            return jsonify({
                "error": "Thi·∫øu th√¥ng tin system_tasks ho·∫∑c daily_report"
            }), 400
        
        # Prompt do user thi·∫øt k·∫ø
        prompt = f"""
H√£y ph√¢n t√≠ch b√°o c√°o h·∫±ng ng√†y so v·ªõi nh·ªØng task ƒëang hi·ªÉn th·ªã tr√™n h·ªá th·ªëng v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng [id] - effort. 
C√°c task ƒëang c√≥ trong h·ªá th·ªëng (ƒë·ªãnh d·∫°ng ID - ProjectName - effort):
{system_tasks_text}
N·ªôi dung b√°o c√°o:
{daily_report}

Khi tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·∫£ v·ªÅ d·∫°ng JSON, kh√¥ng c·∫ßn gi·∫£i th√≠ch. Khi tr·∫£ v·ªÅ k·∫øt qu·∫£ d·ª±a v√†o b√°o c√°o h·∫±ng ng√†y v√† nh·ªØng task ƒëang c√≥ tr√™n h·ªá th·ªëng ƒë·ªÉ tr·∫£ v·ªÅ d·∫°ng ID - EFFORT. N·∫øu trong b√°o c√°o ng√†y kh√¥ng c√≥ ghi effort c·ªßa t·ª´ng task th√¨ l·∫•y s·ªë gi·ªù t·ªïng thu·ªôc v·ªÅ d·ª± √°n chia ƒë·ªÅu cho c√°c task c≈©ng thu·ªôc v·ªÅ d·ª± √°n ƒë√≥. L∆∞u √Ω effort l√† s·ªë gi·ªù l√†m vi·ªác (h). Khi ph√¢n t√≠ch ch·ªâ ph√¢n t√≠ch ph·∫ßn Actual kh√¥ng ph√¢n t√≠ch c√°c ph·∫ßn kh√°c. C√°c ph·∫ßn Other trong Actual c≈©ng kh√¥ng c·∫ßn ph√¢n t√≠ch. V·ªõi nh·ªØng task n√†o kh√¥ng x√°c ƒë·ªãnh tr·∫£ v·ªÅ cho t√¥i v·ªÅ d·∫°ng ProjectName - TaskName. Tr·∫£ v·ªÅ ng√†y th√°ng nƒÉm trong n·ªôi dung b√°o c√°o, ng√†y th√°ng nƒÉm l·∫•y ·ªü nh·ªØng d√≤ng ƒë·∫ßu ti√™n c·ªßa b√°o c√°o
V√≠ d·ª• tr·∫£ v·ªÅ s·∫Ω c√≥ d·∫°ng 
results = "221 - 2\n                  223 - 3". 
undifine = [
{{
"ProjectName": ''
"TaskName:": ''
"effort: "
}},
...]
date - ng√†y th√°ng nƒÉm trong n·ªôi dung b√°o c√°o [yyyy-mm-dd]
"""
        
        # G·ªçi Gemini ƒë·ªÉ sinh k·∫øt qu·∫£
        try:
            from gemini_ai import configure_gemini, generate_natural_language_response
            # S·ª≠ d·ª•ng h√†m generate_natural_language_response ƒë·ªÉ l·∫•y k·∫øt qu·∫£ d·∫°ng text
            # (ho·∫∑c c√≥ th·ªÉ t·∫°o h√†m ri√™ng n·∫øu mu·ªën t√°ch bi·ªát)
            # ·ªû ƒë√¢y ta d√πng model tr·ª±c ti·∫øp ƒë·ªÉ sinh ra k·∫øt qu·∫£
            import google.generativeai as genai
            model = genai.GenerativeModel("gemini-1.5-flash")
            response = model.generate_content(prompt)
            raw = response.text.strip()
            # T√¨m ƒëo·∫°n JSON trong k·∫øt qu·∫£ tr·∫£ v·ªÅ
            import re, json
            json_match = re.search(r'\{.*\}|\[.*\]', raw, re.DOTALL)
            if json_match:
                try:
                    result_json = json.loads(json_match.group(0))
                    return jsonify(result_json)
                except Exception:
                    pass
            # N·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON, tr·∫£ v·ªÅ raw text
            return jsonify({"raw": raw})
        except Exception as e:
            return jsonify({"error": str(e)}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)