import google.generativeai as genai
import re
import logging
from token_utils import token_manager

def configure_gemini(api_key):
    """
    Configure the Gemini API with the provided API key.
    
    C·∫•u h√¨nh API Gemini v·ªõi kh√≥a API ƒë∆∞·ª£c cung c·∫•p.
    """
    genai.configure(api_key=api_key)

def generate_sql_query(question, schema, model_name="gemini-1.5-flash", max_input_tokens=8000):
    """
    Generate SQL query from natural language question using the provided schema.
    
    Args:
        question: User's natural language question
        schema: Database schema text
        model_name: Gemini model name
        max_input_tokens: Maximum input tokens allowed
    
    Returns:
        Generated SQL query string
    
    Raises:
        ValueError: If prompt exceeds token limit
    """
    
    # üîç Token validation v√† optimization
    logging.info(f"Original schema tokens: {token_manager.count_tokens(schema)}")
    
    # Truncate schema if needed
    optimized_schema = token_manager.truncate_schema(schema, question, max_input_tokens - 1000)
    
    prompt = f"""
You are an expert in SQL and assistant for Property Management System (PMS). Based on the following schema:

SECURITY RULES (CRITICAL):
- Generate ONLY SELECT queries for data retrieval
- NO data modification operations allowed
- NO system functions or administrative queries
- Single query only, no chaining with semicolons

SCHEMA:
{optimized_schema}

BUSINESS RULES:
1. Always use CONCAT(firstname, ' ', lastname) AS fullname for displaying and filtering ONLY person names. When filtering by a name, apply conditions on the full name using LOWER(CONCAT(firstname, ' ', lastname)) LIKE '%value%'
2. Exclude sensitive fields: password, ssn, bank_account, internal_notes
3. When filtering by text (e.g., project names), if the value is likely a partial match or pattern (e.g., from user questions), use LIKE '%value%' instead of = 'value'.
4. Use `LOWER(column)` with `LIKE` to ensure case-insensitive matching
5. If the question includes numeric conditions (e.g., greater than, top N, totals), translate it using appropriate aggregate/filtering operators
6. When the user mentions a project name or partial name, search using LOWER(name) with LIKE '%value%' instead of using project ID or subqueries.

QUERY STANDARDS:
Write a single optimized MySQL SELECT query that answers the following user question:
1. Limit to 10 results maximum unless aggregation (e.g., SUM, COUNT) is used
2. Do not select attribute id, created_at, updated_at, or any other attribute that is not useful for the user.
3. Use proper JOINs for related data
4. The question may include pattern matching (e.g., using LIKE with wildcards), filtering, sorting, or joining multiple tables.
5. Always use proper SQL syntax. When using LIKE, include appropriate wildcards (e.g., % or _) if needed for pattern matching.
6. When generating SQL query only use atttribute that is in the schema.

USER QUESTION: "{question}"

Only return the SQL query. No explanation, no markdown, no extra text.
"""
    
    # üîç Validate prompt tr∆∞·ªõc khi g·ªçi API
    is_valid, error_msg = token_manager.validate_prompt(prompt)
    if not is_valid:
        logging.error(f"Prompt validation failed: {error_msg}")
        raise ValueError(f"Token limit exceeded: {error_msg}")
    
    prompt_tokens = token_manager.count_tokens(prompt)
    logging.info(f"Final prompt tokens: {prompt_tokens}")
    
    logging.info(f"Generated SQL prompt: {prompt}")
    model = genai.GenerativeModel(model_name)
    
    try:
        response = model.generate_content(prompt)
        raw = response.text.strip()
        logging.info(f"Raw model output: {raw}")
        
        # Clean the output to remove any markdown formatting
        cleaned = re.sub(r"^```sql\s*|```$", "", raw, flags=re.IGNORECASE).strip()
        logging.info(f"Cleaned SQL: {cleaned}")
        return cleaned
        
    except Exception as e:
        logging.error(f"Error calling Gemini API: {str(e)}")
        raise e

def generate_natural_language_response(question, results, model_name="gemini-1.5-flash", max_token=150, max_input_tokens=4000):
    """
    Generate natural language response from SQL results.
    
    Args:
        question: User's original question
        results: SQL query results
        model_name: Gemini model name
        max_token: Maximum output tokens
        max_input_tokens: Maximum input tokens
    
    Returns:
        Natural language response string
    """
    if not results:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ th·ª≠ h·ªèi l·∫°i theo c√°ch kh√°c kh√¥ng?"
    
    # üîç Optimize results ƒë·ªÉ fit token limit
    optimized_results = token_manager.optimize_results_for_response(results, question, max_input_tokens - 1000)
    
    if len(optimized_results) < len(results):
        logging.info(f"Results optimized: {len(results)} -> {len(optimized_results)} items")
    
    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng k·∫øt qu·∫£ ƒë·ªÉ tr√°nh qu√° d√†i
    sample = optimized_results[:10]
    
    prompt = (
        f"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán v√† chuy√™n nghi·ªáp. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch t·ª± nhi√™n v√† d·ªÖ hi·ªÉu b·∫±ng ti·∫øng Vi·ªát.\n\n"
        f"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {question}\n\n"
        f"D·ªØ li·ªáu t√¨m ƒë∆∞·ª£c: {sample}\n\n"
        "H√£y tr·∫£ l·ªùi theo c√°c nguy√™n t·∫Øc sau:\n"
        "1. S·ª≠ d·ª•ng ng√¥n ng·ªØ t·ª± nhi√™n, th√¢n thi·ªán\n"
        "2. T·ªï ch·ª©c th√¥ng tin m·ªôt c√°ch logic v√† d·ªÖ hi·ªÉu\n"
        "3. N·∫øu c√≥ nhi·ªÅu k·∫øt qu·∫£, h√£y t√≥m t·∫Øt v√† nh·∫•n m·∫°nh th√¥ng tin quan tr·ªçng\n"
        "4. N·∫øu c·∫ßn thi·∫øt, h√£y th√™m c√°c t·ª´ n·ªëi ƒë·ªÉ c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c h∆°n\n"
        "5. Tr√°nh l·∫∑p l·∫°i c√¢u h·ªèi trong c√¢u tr·∫£ l·ªùi\n"
        "6. Gi·ªõi h·∫°n c√¢u tr·∫£ l·ªùi trong kho·∫£ng 150 t·ª´\n"
    )
    
    # üîç Validate prompt tr∆∞·ªõc khi g·ªçi API
    is_valid, error_msg = token_manager.validate_prompt(prompt)
    if not is_valid:
        logging.error(f"Response prompt validation failed: {error_msg}")
        return "Xin l·ªói, d·ªØ li·ªáu qu√° l·ªõn ƒë·ªÉ x·ª≠ l√Ω. Vui l√≤ng th·ª≠ c√¢u h·ªèi c·ª• th·ªÉ h∆°n."
    
    prompt_tokens = token_manager.count_tokens(prompt)
    logging.info(f"Response prompt tokens: {prompt_tokens}")
    
    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt, generation_config={"max_output_tokens": max_token})
        result = response.text.strip()
        if not result:
            return "Xin l·ªói, t√¥i g·∫∑p m·ªôt ch√∫t v·∫•n ƒë·ªÅ khi x·ª≠ l√Ω c√¢u tr·∫£ l·ªùi. B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i kh√¥ng?"
        return result
    except Exception as e:
        logging.error(f"Error generating natural response: {str(e)}")
        return "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i sau."
